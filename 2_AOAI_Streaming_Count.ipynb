{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get tokens usage (Stream=True)\n",
    "In this notebook, you'll analyse token usage for Chat Completions with enabled Streaming.\n",
    "\n",
    "### Setup\n",
    "Load the API key and relevant Python libaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import openai\n",
    "import os\n",
    "\n",
    "# Define Azure OpenAI endpoint parameters\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_version = \"2023-07-01-preview\"\n",
    "openai.api_base = os.getenv(\"OPENAI_API_BASE\") # Set AOAI endpoint value as env variable\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\") # Set AOAI API key as env variable\n",
    "aoai_deployment = os.getenv(\"OPENAI_API_DEPLOY\") # Set AOAI deployment name as env variable"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function: Completions\n",
    "This helper function will make it easier to use prompts and retrieve chat completion outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function\n",
    "def get_completion(prompt, model=\"gpt-3.5-turbo\", engine=aoai_deployment):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        # model=model, # used by original OpenAI endpoint\n",
    "        engine=engine, # used by Azure OpenAI endpoint\n",
    "        messages=messages,\n",
    "        temperature=0, # this is the degree of randomness of the model's output\n",
    "        stream=True, # this parameter can enable streaming\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function: Tokens Count\n",
    "This helper function will help with token count using OpenAI's tiktoken library.\n",
    "\n",
    "It's based on the OpenAI's original sample code from this GitHub repo: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "def num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0613\"):\n",
    "    \"\"\"\n",
    "    Return the number of tokens used by a list of messages.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        print(\"Warning: model not found. Using cl100k_base encoding.\")\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "    if model in {\n",
    "        \"gpt-3.5-turbo-0613\",\n",
    "        \"gpt-3.5-turbo-16k-0613\",\n",
    "        \"gpt-4-0314\",\n",
    "        \"gpt-4-32k-0314\",\n",
    "        \"gpt-4-0613\",\n",
    "        \"gpt-4-32k-0613\",\n",
    "        }:\n",
    "        tokens_per_message = 3\n",
    "        tokens_per_name = 1\n",
    "    elif model == \"gpt-3.5-turbo-0301\":\n",
    "        tokens_per_message = 4  # every message follows <|start|>{role/name}\\n{content}<|end|>\\n\n",
    "        tokens_per_name = -1  # if there's a name, the role is omitted\n",
    "    elif \"gpt-3.5-turbo\" in model:\n",
    "        print(\"Warning: gpt-3.5-turbo may update over time. Returning num tokens assuming gpt-3.5-turbo-0613.\")\n",
    "        return num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0613\")\n",
    "    elif \"gpt-4\" in model:\n",
    "        print(\"Warning: gpt-4 may update over time. Returning num tokens assuming gpt-4-0613.\")\n",
    "        return num_tokens_from_messages(messages, model=\"gpt-4-0613\")\n",
    "    else:\n",
    "        raise NotImplementedError(\n",
    "            f\"\"\"num_tokens_from_messages() is not implemented for model {model}. See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.\"\"\"\n",
    "        )\n",
    "    \n",
    "    num_tokens = 0\n",
    "\n",
    "    if type(messages) == list:\n",
    "        for message in messages:\n",
    "            num_tokens += tokens_per_message\n",
    "            for key, value in message.items():\n",
    "                num_tokens += len(encoding.encode(value))\n",
    "                if key == \"name\":\n",
    "                    num_tokens += tokens_per_name\n",
    "        num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>\n",
    "    elif type(messages) == str:\n",
    "        num_tokens += len(encoding.encode(messages))\n",
    "    return num_tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Completions\n",
    "Getting output of ChatCompletion API in chunks with enabled Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The solar system is a collection of celestial bodies that orbit around the Sun. It consists of the Sun, eight planets, their moons, asteroids, comets, and other smaller objects. The Sun is at the center of the solar system and provides heat, light, and energy to all the other bodies.\n",
      "\n",
      "The eight planets in the solar system, in order of their distance from the Sun, are Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and Neptune. These planets vary in size, composition, and atmospheric conditions. Mercury is the smallest planet, while Jupiter is the largest. Earth is the only known planet to support life.\n",
      "\n",
      "Each planet has its own unique characteristics. For example, Venus has a thick atmosphere and a runaway greenhouse effect, making it the hottest planet in the solar system. Mars is known as the \"Red Planet\" due to its reddish appearance caused by iron oxide on its surface. Jupiter is a gas giant and has a prominent system of rings and numerous moons, including the largest moon in the solar system, Ganymede.\n",
      "\n",
      "Apart from planets, the solar system also contains numerous moons. Earth has one moon, while other planets have multiple moons. For example, Saturn has over 80 known moons, including Titan, which is larger than the planet Mercury.\n",
      "\n",
      "The solar system also contains asteroids, which are rocky objects that orbit the Sun, mostly found in the asteroid belt between Mars and Jupiter. Comets, on the other hand, are icy bodies that originate from the outer regions of the solar system and have highly elliptical orbits.\n",
      "\n",
      "The study of the solar system is known as astronomy, and it has been a subject of fascination and exploration for centuries. Space missions, telescopes, and other scientific instruments have provided valuable information about the solar system, its formation, and its evolution."
     ]
    }
   ],
   "source": [
    "prompt = \"Tell me about Solar system\"\n",
    "\n",
    "response = get_completion(prompt)\n",
    "result = []\n",
    "\n",
    "for chunk in response:\n",
    "    try:\n",
    "        content = chunk[\"choices\"][0].get(\"delta\", {}).get(\"content\")\n",
    "        if content:\n",
    "            print(content, end=\"\")\n",
    "            result.append(content)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokens usage details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiktoken-calculated token usage:\n",
      "- Prompt tokens: 12\n",
      "- Completion tokens: 366\n",
      "- Total tokens: 378\n"
     ]
    }
   ],
   "source": [
    "tiktoken_prompt = [{\"role\": \"user\", \"content\": prompt}]\n",
    "tiktoken_prompt_tokens = num_tokens_from_messages(tiktoken_prompt)\n",
    "tiktoken_completion_tokens = num_tokens_from_messages(\"\".join(result))\n",
    "\n",
    "print(\"Tiktoken-calculated token usage:\")\n",
    "print(f\"- Prompt tokens: {tiktoken_prompt_tokens}\")\n",
    "print(f\"- Completion tokens: {tiktoken_completion_tokens}\")\n",
    "print(f\"- Total tokens: {tiktoken_prompt_tokens + tiktoken_completion_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
